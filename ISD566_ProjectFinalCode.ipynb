{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvb-3YbqYPSP"
   },
   "outputs": [],
   "source": [
    "# Nikita Bawane, Ritu Gangwal, Utkarsh Ujwal\n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "\n",
    "#Basic\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "! pip install -U gensim\n",
    "import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "import string \n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "! pip install plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLfqNfK5nIsG"
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "3npu53TuW0YD",
    "outputId": "f0446cd6-92e1-45f3-cbbc-f6d57fcf180c"
   },
   "outputs": [],
   "source": [
    "#Loading the training data\n",
    "train = pd.read_csv(r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Project\\Code\\train.csv')\n",
    "\n",
    "# loading the test data\n",
    "test = pd.read_csv(r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Project\\Code\\test.csv')\n",
    "test.head()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_rpP8brTJrI"
   },
   "source": [
    "**##########################################################################**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS5vSOjPyCyn"
   },
   "source": [
    "We see that their are 6 levels of toxicity in the training dataset:\n",
    "\n",
    "Toxic, Severe toxic, Obscene, Threat, Insult, Identity Hate\n",
    "\n",
    "These seem to be in the increasing level of toxicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Peat9WGXx_KI"
   },
   "source": [
    "Let's look at some random comments from each toxicity class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "np39nYZex2VP",
    "outputId": "dc0e3fc3-6725-452c-eb9b-239ded20d1a3"
   },
   "outputs": [],
   "source": [
    "toxic_label = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for toxicity in toxic_label:\n",
    "    rand_text = np.random.choice(train[train[toxicity]==1].index,size=1)[0] # Select a random comment from each class\n",
    "    print('Let us see a comment of {} text\\n'.format(toxicity), train.iloc[rand_text,1],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOiAo1NQzPWc"
   },
   "source": [
    "No. of toxic and non-toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BflC8einy6g3",
    "outputId": "bdf48893-63c3-43f0-f45c-3f6e6b77607c"
   },
   "outputs": [],
   "source": [
    "count_labels = train.select_dtypes(include=np.number).sum(axis=0)\n",
    "total_toxic_comments = train.select_dtypes(include=np.number).apply(lambda row: any(row) == 1, axis=1).sum()\n",
    "print('Total number of Toxic texts = {}, out of {}'.format(total_toxic_comments, train.shape[0]))\n",
    "print('Total number of Non-Toxic comments: {}, out of {}'.format(train.shape[0] - total_toxic_comments, train.shape[0]))\n",
    "count_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spd0TcQ6zW8j"
   },
   "source": [
    "Plotting the graph for toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "FnmvnwgTy7Iz",
    "outputId": "d0fe6720-b266-4022-fb20-9d49e6e4201e"
   },
   "outputs": [],
   "source": [
    "dist_plot = plt.figure(figsize=(12,6))\n",
    "_=sns.barplot(x=count_labels.index,y=count_labels)\n",
    "_=plt.xlabel('Toxicity Class')\n",
    "_=plt.ylabel('Occurance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MZk1INE68wd"
   },
   "source": [
    "We observe that the dataset is imbalanced. We have only considered those comments whhere the flag for toxicity of even one class is set to 1 and Still we observe much more Toxic comments than Servere toxic or Threat\n",
    "\n",
    "Further exploratory shows that label toxic has the most observations in the training dataset while threat has the least.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEUwo7ZIPhez",
    "outputId": "442ed3ba-1a4e-4bf9-c568-740e8286fbb8"
   },
   "outputs": [],
   "source": [
    "# Adding 'none' columns = if all zero's than zero else 1\n",
    "train['none'] = (train[toxic_label].max(axis=1) == 1).astype(int)\n",
    "toxic_label.append('none')\n",
    "train.head(10)\n",
    "toxic_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1AmtK_JENh_"
   },
   "source": [
    "**Lets's check the correlation between various toxic comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "iA_MmboH9WSN",
    "outputId": "406db970-1d09-44d6-e924-b49fafcaa0cb"
   },
   "outputs": [],
   "source": [
    "rows = [{l:train[f].corr(train[l]) for l in toxic_label} for f in toxic_label]\n",
    "train_corr = pd.DataFrame(rows, index=toxic_label)\n",
    "train_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_BqgdcrSwYg"
   },
   "source": [
    "### Lets's check the correlation of these new features we have created and see if these assumption even hold true in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "Cllv1mWaHJ9F",
    "outputId": "b24d4a6a-6238-41ca-edd8-9beface76d9f"
   },
   "outputs": [],
   "source": [
    "# Let's make a heatmap for this correlation matrix\n",
    "import seaborn as sns\n",
    "corr_feature = sns.heatmap(train_corr, vmin=-1, vmax=1, center=0.0, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEm0rSh2N2ty"
   },
   "source": [
    "The correlation matrix shows interesting things :\n",
    "\n",
    "'toxic' is clearly correlated with 'obscene' and 'insult' (0.68 and 0.65)\n",
    "\n",
    "'toxic' and 'severe_toxic' are only got a 0.31 correlation factor\n",
    "\n",
    "'insult' and 'obscene' have a correlation factor of 0.74\n",
    "\n",
    "From my point of view, there are several combinations that are worth digging into :\n",
    "\n",
    "'toxic' <-> 'severe_toxic'. The semantic of these two categories seems to show some kind of graduation between them\n",
    "\n",
    "'toxic' <-> 'insult' and 'toxic' <-> 'obscene'\n",
    "\n",
    "'insult' <-> 'obscene'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asM6yQHmSe3D"
   },
   "source": [
    "### Let's check some assumpation which we have about the toxic comments we witness on social media and see if they have any correlation with the \"Toxicity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlTaYkoDSiXL"
   },
   "outputs": [],
   "source": [
    "# Let's make a data frame copy\n",
    "df_1 = train.copy()\n",
    "\n",
    "# Total length of the comment text...\n",
    "df_1['total_length'] = df_1['comment_text'].apply(len)\n",
    "\n",
    "#Generally the people who are angry and write toxic comment use Capital letter words\n",
    "df_1['CAPITAL_WORDS'] = df_1['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "df_1['CAPS_LENGTH'] = df_1.apply(lambda row: float(row['CAPITAL_WORDS'])/float(row['total_length']),axis=1)\n",
    "\n",
    "# Toxic comments generally contain exclaimation marks (way to depict emotions)\n",
    "df_1['exclamation_marks'] = df_1['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "\n",
    "# People who are angry and spread toxicity generally do engage in a civil discussions and ask questions\n",
    "df_1['question_marks'] = df_1['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "\n",
    "# Angry and toxic people don't really care about proper grammar and hence punctuations\n",
    "df_1['punctuation'] = df_1['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "\n",
    "# Angry and toxic people generally tend to repet the words\n",
    "df_1['unique_words'] = df_1['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "4slYZScaSymE",
    "outputId": "489e0e89-c9ee-49d7-cd8c-25cf62f08812"
   },
   "outputs": [],
   "source": [
    "features = ('total_length', 'CAPITAL_WORDS', 'CAPS_LENGTH', 'exclamation_marks',\n",
    "           'question_marks', 'punctuation','unique_words')\n",
    "labels = ('toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate')\n",
    "rows = [{l:df_1[f].corr(df_1[l]) for l in labels} for f in features]\n",
    "df_1_corr = pd.DataFrame(rows, index=features)\n",
    "df_1_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "mh5t_kVgS6Hf",
    "outputId": "343e839e-cc16-4106-9137-b007e54499a7"
   },
   "outputs": [],
   "source": [
    "# Let's make a heatmap for this correlation matrix\n",
    "import seaborn as sns\n",
    "corr_extrafeature = sns.heatmap(df_1_corr, vmin= -0.1, vmax= 0.1, center=0.0,annot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGTx-CAaTFpF"
   },
   "source": [
    "From the above heatmap we can conclude that a few of our assumptions are kind of accurate:\n",
    "\n",
    "People do use CAPITAL letter words whne writing a toxic comment as we observe a positive correlation.\n",
    "\n",
    "People don't really use proper grammar (eg punctuation) or unique words while being toxic.\n",
    "\n",
    "Poeple tend to use exclamation (!!!) while angry to show the severity of their emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoxUsmGmlXUM"
   },
   "source": [
    "**Checking if the Toxic comments are longer or the Non-Toxic ones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "vcU94Y5PlurD",
    "outputId": "52ad15c0-bf60-4f1e-ffea-e25bb7c48a0b"
   },
   "outputs": [],
   "source": [
    "train['comment_length'] = train['comment_text'].apply(len)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc_CacWjDN2f"
   },
   "source": [
    "Below is the plot showing the comment length frequency. As noticed, most of the comments are short with only a few comments longer than 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "14PgNTELCumA",
    "outputId": "f3ba17a1-076e-4175-f977-536ddc2ee487"
   },
   "outputs": [],
   "source": [
    "sns.distplot(train['comment_length'], kde=False, bins=20, color=\"steelblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "Hc1DQJc7l7tY",
    "outputId": "7f18bade-777a-40c7-99dd-8f70c84bcd61"
   },
   "outputs": [],
   "source": [
    "# Let's visualize it\n",
    "sns.set(palette='rocket')\n",
    "plot, plotobj = plt.subplots(1,2,sharex=True, figsize=(15,6))\n",
    "_=sns.distplot(np.log10(train.loc[train.toxic==1,'comment_length']),kde=False, bins=15,plotobj=plotobj[0])\n",
    "_=plotobj[0].set_xlabel('log (Character length of comments)')\n",
    "_=plotobj[0].set_ylabel('Count')\n",
    "_=plotobj[0].set_title('Toxic comments')\n",
    "\n",
    "_=sns.distplot(np.log10(train.loc[train.toxic==0,'comment_length']),kde=False, bins=15,plotobj=plotobj[1])\n",
    "_=plotobj[1].set_xlabel('log (Character length of comments)')\n",
    "_=plotobj[1].set_title('Non-Toxic comments')\n",
    "\n",
    "print('Mean character length of toxic comments: {}'.format(train.loc[train.toxic==1,'comment_length'].mean()))\n",
    "print('Mean character length of clean comments: {}'.format(train.loc[train.toxic==0,'comment_length'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "pqUCjCeDBBZZ",
    "outputId": "f2ffa0e3-fe51-4448-d53a-d3d412054ae1"
   },
   "outputs": [],
   "source": [
    "# word clouds\n",
    "def W_Cloud(token):\n",
    "# Visualize the most common words contributing to the token.\n",
    "    threat_context = train[train[token] == 1]\n",
    "    threat_text = threat_context.comment_text\n",
    "    neg_text = pd.Series(threat_text).str.cat(sep=' ')\n",
    "    wordcloud = WordCloud(width=1600, height=800,\n",
    "                          max_font_size=200).generate(neg_text)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(wordcloud.recolor(colormap=\"Blues\"), interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Most common words assosiated with {token} comment\", size=20)\n",
    "    plt.show()\n",
    "\n",
    "W_Cloud('identity_hate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4atCYyPVpNY"
   },
   "source": [
    "##Let's do Topic Modelling using LDA and plot the t-SNE to view the result in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwtG9zHVV6RI"
   },
   "source": [
    "We won't distinguish between the various categories of toxicity. We'll mark comments from any of the six categories. To make it a little more interesting, our target variable will show the number of categories observed for each comment. For example, a comment classified as toxic, severe_toxic and obscene gets a 3. I'll color the t-SNE plot to highlight comments falling in 1 or more categories.\n",
    "\n",
    "I took 20% of the data to reduce the clutter of the picture and speed things up. Running the whole train set shows similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNWdq1HPV71P"
   },
   "outputs": [],
   "source": [
    "trainX = train['comment_text']\n",
    "target = train.sum(axis=1).values\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, train_size=0.20)\n",
    "for train_index, test_index in sss.split(trainX, target):\n",
    "    train_text = trainX.iloc[train_index] \n",
    "    train_tgt = target[train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hoxlo520WMB0"
   },
   "source": [
    "**Now we can count the word features and run LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9f5gWE-WLGm"
   },
   "outputs": [],
   "source": [
    "maxfeats = 5000\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=maxfeats)\n",
    "word_vectorizer.fit(train_text)\n",
    "train_features = word_vectorizer.transform(train_text)\n",
    "\n",
    "classifier = LatentDirichletAllocation(n_components=16, learning_method= 'batch', n_jobs=3, verbose=1)\n",
    "train_lda = classifier.fit_transform(train_features, train_tgt)\n",
    "train_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ASz-ouCWae5"
   },
   "source": [
    "We can represent the topic vectors in 2-d with t-SNE. Finally we plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zFvvGONWbiA"
   },
   "outputs": [],
   "source": [
    "## T-SNE (T-distributed stochastic neighbour embedding)\n",
    "\n",
    "tsne_obj = TSNE(n_components=2, perplexity=8, n_iter=1000, verbose=1, angle=0.5) #perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms\n",
    "train_tsne = tsne_obj.fit_transform(train_lda)\n",
    "x_tsne = train_tsne[:, 0]\n",
    "y_tsne = train_tsne[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-noBvVMMWyU0"
   },
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=False)\n",
    "\n",
    "# create datafRAME WITH comments, target, tsnex,tsney\n",
    "#separate into 2 groups of x_nice, x_notnice, y_nice, y_notnice\n",
    "plot = pd.DataFrame({'comment':train_text, 'class':train_tgt, 'x_axis': x_tsne, 'y_axis':y_tsne})\n",
    "nonToxic = plot[plot['class'] == 0]\n",
    "Toxic = plot[plot['class'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VKbY6NbW9L1"
   },
   "outputs": [],
   "source": [
    "nonToxic_Plot = Scatter(\n",
    "    x = nonToxic['x_axis'],\n",
    "    y = nonToxic['y_axis'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "      size=7,\n",
    "      color='lightgray',\n",
    "      symbol='circle',\n",
    "      line = dict(width = 0,\n",
    "        color='gray'),\n",
    "      opacity = 0.3\n",
    "     ),\n",
    "    text=nonToxic['comment']\n",
    ")\n",
    "\n",
    "Toxic_Plot = Scatter(\n",
    "    x = Toxic['x_axis'],\n",
    "    y = Toxic['y_axis'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "      size=8,\n",
    "      color=notnices['class'],\n",
    "      symbol='triangle-up',\n",
    "      line = dict(width = 0,\n",
    "        color='Darkred'),\n",
    "      opacity = 0.6\n",
    "     ),\n",
    "    text=Toxic['comment']\n",
    ")\n",
    "\n",
    "data=[nonToxic_Plot, Toxic_Plot]\n",
    "\n",
    "layout = Layout(\n",
    "    title = 'I see you Haters!!!!',\n",
    "    showlegend=False,\n",
    "    xaxis=dict(\n",
    "        autorange=True,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        \n",
    "        \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        autorange=True,\n",
    "        showgrid=True,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "     )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCVATZdYsAv-"
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly import version\n",
    "print (version)\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "from IPython.display import HTML\n",
    "LDA_Plot = Figure(data=data, layout=layout)\n",
    "HTML(LDA_Plot.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJzNdgqepQEc"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hNN3Q3kxKD3",
    "outputId": "aea0bc92-ea7f-409d-e173-dacc2b9edbd1"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'([a-zA-Z]+)') \n",
    "\n",
    "def get_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def preprocess_comment(input_str):\n",
    "    comment=re.sub(r\"(\\d{1,3}\\.){1,3}\\d{1,3}\",\"\",input_str)\n",
    "    words = [word for word in tokenizer.tokenize(comment.lower()) if not word in stop_words]\n",
    "    lemmatized_sentence = ' '.join([lemmatizer.lemmatize(word,pos = get_pos(word)) for word in words if len(word)>2])\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bg1nr_XyKuc",
    "outputId": "c1742354-7d2c-4a66-991e-e46ac7d6ddb1"
   },
   "outputs": [],
   "source": [
    "# An example\n",
    "print('Original text:\\n {}'.format(train['comment_text'][20]))\n",
    "print('After cleaning:\\n {}'.format(preprocess_comment(train['comment_text'][20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "KDGg12N71L70",
    "outputId": "02cfc11e-01e6-4e5f-99be-0cfbecab69c8"
   },
   "outputs": [],
   "source": [
    "train['cleaned_comment_text']= train['comment_text'].apply(lambda comment: preprocess_comment(comment))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "_7mryqEN1Ob_",
    "outputId": "1d3387a4-98fd-44d8-9aaf-475e2fb5807b"
   },
   "outputs": [],
   "source": [
    "test['cleaned_comment_text'] = test['comment_text'].apply(lambda comment: preprocess_comment(comment))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRBru86diT8N"
   },
   "source": [
    "Applyting sentiment analysis - Text blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "6sv3ajreiSTM",
    "outputId": "08d29b65-48f5-4249-dc63-7bd917d6df04"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "#Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "  return TextBlob(text).polarity\n",
    "\n",
    "def getSentiment(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'\n",
    "\n",
    "train['TextBlob_Score'] = train['cleaned_comment_text'].apply(getPolarity)\n",
    "train['TextBlob_Sentiment'] = train['TextBlob_Score'].apply(getAnalysis)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bel3EItLk-yI"
   },
   "source": [
    "VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "Ff0PpD6UiR3z",
    "outputId": "f886cc4b-8ace-4edb-9fe4-9588b11c1fec"
   },
   "outputs": [],
   "source": [
    "# Vader sentiment analysis\n",
    "SentAnaylzer = SentimentIntensityAnalyzer()\n",
    "def getVaderScore(sent):\n",
    "  return SentAnaylzer.polarity_scores(sent)[\"compound\"]\n",
    "\n",
    "def getVaderSentiment(score):\n",
    "    if score >= 0.05:\n",
    "      return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "      return 'Negative'\n",
    "    else:\n",
    "      return 'Neutral'\n",
    "\n",
    "train['Vader_Score'] = train['cleaned_comment_text'].apply(getVaderScore)\n",
    "train['Vader_Sentiment'] = train['Vader_Score'].apply(getVaderSentiment)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "id": "TL1HTr45o39w",
    "outputId": "6b3ef12d-4e60-440f-e86b-9b75d9aa4839"
   },
   "outputs": [],
   "source": [
    "def toxic(sentiment):\n",
    "    if sentiment == 'Negative':\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "train['TextBlob_Toxic'] = train['TextBlob_Sentiment'].apply(toxic)\n",
    "train['Vader_Toxic'] = train['Vader_Sentiment'].apply(toxic)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sNmvDoVUrBB9",
    "outputId": "3affbc18-a9e9-4264-911c-d0a9eb5bb771"
   },
   "outputs": [],
   "source": [
    "# Calculating accuracies of both methods\n",
    "# accuracy of textblob\n",
    "accuracy1 = 0\n",
    "for i in range(len(train)):\n",
    "  if train['none'][i] == train['TextBlob_Toxic'][i]:\n",
    "    accuracy1 += 1\n",
    "Accuracy1 = (accuracy1 / len(train))*100\n",
    "print('Accuracy of TextBlob: = {}'.format(Accuracy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsTXIZCK2Zxd",
    "outputId": "768a9f88-750e-4d5e-dde2-b4e650205bcf"
   },
   "outputs": [],
   "source": [
    "# accuracy of Vader\n",
    "accuracy2 = 0\n",
    "for i in range(len(train)):\n",
    "  if train['none'][i] == train['Vader_Toxic'][i]:\n",
    "    accuracy2 += 1\n",
    "Accuracy2 = (accuracy2 / len(train))*100\n",
    "print('Accuracy of Vader: = {}'.format(Accuracy2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08HZKsbL3SZI"
   },
   "source": [
    "Since accuracy of Vader (73.58%) < TextBlob (77.37%), we will continue with TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBchYKjkGUoS"
   },
   "source": [
    "Making a new training set with rows where textblob sentiment = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "id": "c6TjpIk_GT_3",
    "outputId": "71fa67a7-7bbf-4702-f6e1-98e325452380"
   },
   "outputs": [],
   "source": [
    "train_new = train.loc[train.iloc[:,train.columns.get_loc('TextBlob_Toxic')] == 1] \n",
    "\n",
    "# dropping uneccesary columns\n",
    "train_new = train_new.drop(['Vader_Score','Vader_Sentiment','Vader_Toxic','TextBlob_Score','TextBlob_Sentiment','TextBlob_Toxic'], axis=1)\n",
    "train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tN_MJc4YGTk0",
    "outputId": "1045fc5d-abcc-4c1e-a37d-9f95badbc8ce"
   },
   "outputs": [],
   "source": [
    "# After applying sentiment analysis, we got a new training set with 37072 rows\n",
    "train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5u9PATGWMHU"
   },
   "source": [
    "Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3YiPS0Cahg8"
   },
   "outputs": [],
   "source": [
    "# Creating classifiers with default parameters initially.\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = LogisticRegression(max_iter=10000)\n",
    "clf3 = LinearSVC(max_iter=10000, dual=False)\n",
    "clf4 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGiciR4EYy9V"
   },
   "source": [
    "Feature Vectorization - Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwTYwcd1sLlf"
   },
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=30,ngram_range=(1, 2),\n",
    "                                   analyzer='word',strip_accents='unicode',\n",
    "                                   min_df=10, max_df = 200)  \n",
    "\n",
    "X_train_count = count_vectorizer.fit_transform(train_new['cleaned_comment_text'])\n",
    "X_test_count = count_vectorizer.transform(test['cleaned_comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA9_2OD6XTv5"
   },
   "outputs": [],
   "source": [
    "# TFIDF Vectorizer\n",
    "tfidf_vector = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),          #Consier both unigrams and bi-grams\n",
    "    analyzer='word',\n",
    "    strip_accents='unicode', \n",
    "    use_idf=1, \n",
    "    min_df=10)\n",
    "\n",
    "X_train_tfidf = tfidf_vector.fit_transform(train_new['cleaned_comment_text'])\n",
    "X_test_tfidf = tfidf_vector.transform(test['cleaned_comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieupMLjbWVAy"
   },
   "outputs": [],
   "source": [
    "# getting top 20 words\n",
    "def get_topn_word_counts(vectorizer, feature_counts, n=10):\n",
    "    count_docs = feature_counts.A.sum(axis=0)\n",
    "    count_feature_names = vectorizer.get_feature_names()\n",
    "    top_words_counts = sorted(zip(count_feature_names, count_docs), key= lambda x:x[1], reverse=True)\n",
    "    word_count = pd.DataFrame(top_words_counts, columns = ['token', 'count'])[:n]\n",
    "    return word_count\n",
    "\n",
    "top_20_wordcounts_count = get_topn_word_counts(count_vectorizer, X_train_count, n=20)\n",
    "top_20_wordcounts_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ha0YAP0W7p2"
   },
   "outputs": [],
   "source": [
    "# plotting top 20 words graph\n",
    "twentywordplot = plt.figure(figsize=(20,10))\n",
    "_ = sns.barplot(x=\"token\", y=\"count\", data=top_20_wordcounts_count)\n",
    "_=plt.title('Top 20 words in the corpus for count vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86f3Uk9Xr1Q3"
   },
   "outputs": [],
   "source": [
    "# tfidf top 20 words\n",
    "top_20_wordcounts_tfidf = get_topn_word_counts(tfidf_vector, X_train_tfidf, n=20)\n",
    "top_20_wordcounts_tfidf\n",
    "\n",
    "# plotting top 20 words graph\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "_ = sns.barplot(x=\"token\", y=\"count\", data=top_20_wordcounts_tfidf)\n",
    "_=plt.title('Top 20 words in the corpus for tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpfSGeuKg27z"
   },
   "source": [
    "Checking on training data - Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "indwKbPHUWUF"
   },
   "outputs": [],
   "source": [
    "# With CountVectorizer and TFIDF\n",
    "test_labels = [\"toxic\", \"severe_toxic\", \"obscene\",\"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "def cross_validation_score(classifier, X_train, y_train):\n",
    "    #Iterate though each label and return the cross validation F1, Recall and accuracy score \n",
    "    methods = []\n",
    "    name = classifier.__class__.__name__.split('.')[-1]\n",
    "    for label in test_labels:\n",
    "        recall = cross_val_score(classifier, X_train, y_train[label], cv=10, scoring='recall')\n",
    "        f1 = cross_val_score(classifier, X_train,y_train[label], cv=10, scoring='f1')\n",
    "        accuracy = cross_val_score(classifier, X_train,y_train[label], cv=10, scoring='accuracy')\n",
    "        methods.append([name, label, recall.mean(), f1.mean(), accuracy.mean()])\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM2wbxTKUhMw"
   },
   "outputs": [],
   "source": [
    "# Calculating the cross validation F1 and Recall score for our 4 baseline models with CountVectorizer.\n",
    "methods1_cv_count = pd.DataFrame(cross_validation_score(clf1, X_train_count, train_new))\n",
    "methods2_cv_count = pd.DataFrame(cross_validation_score(clf2, X_train_count, train_new))\n",
    "methods3_cv_count = pd.DataFrame(cross_validation_score(clf3, X_train_count, train_new))\n",
    "#methods4_cv_count = pd.DataFrame(cross_validation_score(clf4, X_train_count, train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuL15Jl2UsoC"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe to show summary of results of CountVectorizer\n",
    "methods_cv_count = pd.concat([methods1_cv_count, methods2_cv_count, methods3_cv_count]) #, methods4_cv_count])\n",
    "methods_cv_count.columns = ['Model', 'Label', 'Recall', 'F1','Accuracy']\n",
    "meth_cv_count = methods_cv_count.reset_index()\n",
    "meth_cv_count[['Model', 'Label', 'Recall', 'F1','Accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X68ChX4XY8lt"
   },
   "source": [
    "Feature Vectorization - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW3EPTMFbvbV"
   },
   "outputs": [],
   "source": [
    "# Calculating the cross validation F1 and Recall score for our 4 baseline models with TF-IDF.\n",
    "methods1_cv_tfidf = pd.DataFrame(cross_validation_score(clf1, X_train_tfidf, train_new))\n",
    "methods2_cv_tfidf = pd.DataFrame(cross_validation_score(clf2, X_train_tfidf, train_new))\n",
    "methods3_cv_tfidf = pd.DataFrame(cross_validation_score(clf3, X_train_tfidf, train_new))\n",
    "#methods4_cv_tfidf = pd.DataFrame(cross_validation_score(clf4, X_train_tfidf, train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k--WUv59bwlG"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe to show summary of results of TF-IDF\n",
    "methods_cv = pd.concat([methods1_cv_tfidf, methods2_cv_tfidf, methods3_cv_tfidf]) #, methods4_cv_tfidf])\n",
    "methods_cv.columns = ['Model', 'Label', 'Recall', 'F1','Accuracy']\n",
    "meth_cv_tfidf = methods_cv.reset_index()\n",
    "meth_cv_tfidf[['Model', 'Label', 'Recall', 'F1','Accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7_u7JyTg90d"
   },
   "source": [
    "On test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzTkJakag9Uk"
   },
   "outputs": [],
   "source": [
    "# reading the test lables\n",
    "test_y = pd.read_csv(r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Project\\Code\\test_labels.csv')\n",
    "test_y.head()\n",
    "#len(test_y)  =  153164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntWLhRrtg9C0"
   },
   "outputs": [],
   "source": [
    "def score(classifier, X_train, y_train, X_test, y_test):\n",
    "    #Calculate F1, Recall for each label on test dataset.\n",
    "    methods = []\n",
    "    name = classifier.__class__.__name__.split('.')[-1]\n",
    "    predict_df = pd.DataFrame()\n",
    "    predict_df['id'] = test_y['id']\n",
    "\n",
    "    for label in test_labels:\n",
    "        classifier.fit(X_train, y_train[label])\n",
    "        predicted = classifier.predict(X_test)\n",
    "        predict_df[label] = predicted\n",
    "        recall = recall_score(y_test[y_test[label] != -1][label],predicted[y_test[label] != -1],average=\"weighted\")\n",
    "        f1 = f1_score(y_test[y_test[label] != -1][label],predicted[y_test[label] != -1],average=\"weighted\")\n",
    "        accuracy = accuracy_score(y_test[y_test[label] != -1][label],predicted[y_test[label] != -1])\n",
    "        conf_mat = confusion_matrix(y_test[y_test[label] != -1][label],predicted[y_test[label] != -1])\n",
    "        methods.append([name, label, recall, f1, accuracy, conf_mat])\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujNhZpt7gTjS"
   },
   "source": [
    "Testing for count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYdo_odKg8nw"
   },
   "outputs": [],
   "source": [
    "# Calculating the F1 and Recall score for our 4 models.\n",
    "method1 = pd.DataFrame(score(clf1, X_train_count, train_new, X_test_count, test_y))\n",
    "method2 = pd.DataFrame(score(clf2, X_train_count, train_new, X_test_count, test_y))\n",
    "method3 = pd.DataFrame(score(clf3, X_train_count, train_new, X_test_count, test_y))\n",
    "#method4 = pd.DataFrame(score(clf4, X_train_count, train_new, X_test_count, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xGuWELDkpCH"
   },
   "outputs": [],
   "source": [
    "methods = pd.concat([method1, method2, method3]) #, method4])\n",
    "methods.columns = ['Model', 'Label', 'Recall', 'F1', 'Accuracy','Confusion_Matrix']\n",
    "meth = methods.reset_index()\n",
    "meth[['Model', 'Label', 'Recall', 'F1', 'Accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpF3L8bBnuNI"
   },
   "outputs": [],
   "source": [
    "# Visualizing F1 score results through box-plot.\n",
    "ax = sns.boxplot(x='Model', y='Accuracy', data=methods, palette=\"Blues\")\n",
    "sns.stripplot(x='Model', y='Accuracy', data=methods, size=8, jitter=True, edgecolor=\"gray\", linewidth=2, palette=\"Blues\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvtT7NYZgY_S"
   },
   "source": [
    "Testing for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P8LVlyFgclG"
   },
   "outputs": [],
   "source": [
    "# Calculating the F1 and Recall score for our 3 baseline models.\n",
    "method1t = pd.DataFrame(score(clf1, X_train_tfidf, train_new, X_test_tfidf, test_y))\n",
    "method2t = pd.DataFrame(score(clf2, X_train_tfidf, train_new, X_test_tfidf, test_y))\n",
    "method3t = pd.DataFrame(score(clf3, X_train_tfidf, train_new, X_test_tfidf, test_y))\n",
    "#method4t = pd.DataFrame(score(clf4, X_train_tfidf, train_new, X_test_tfidf, test_y))\n",
    "\n",
    "methods_t = pd.concat([method1t, method2t, method3t]) #, method4t])\n",
    "methods_t.columns = ['Model', 'Label', 'Recall', 'F1', 'Accuracy','Confusion_Matrix']\n",
    "meth_t = methods_t.reset_index()\n",
    "meth_t[['Model', 'Label', 'Recall', 'F1', 'Accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVHRa46yDxM1"
   },
   "source": [
    "## Let's Explore another embedding algorithm \"Word2Vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1uTcOHWD6l0"
   },
   "outputs": [],
   "source": [
    "traindata_copy = train_new.copy()\n",
    "split = 0.7\n",
    "d_train = traindata_copy[:int(split*len(traindata_copy))]\n",
    "d_val = traindata_copy[int((1-split)*len(traindata_copy)):]\n",
    "\n",
    "stop_words = set(['all', \"she'll\", \"don't\", 'being', 'over', 'through', \n",
    "'yourselves', 'its', 'before', \"he's\", \"when's\", \"we've\", 'had', 'should',\n",
    "\"he'd\", 'to', 'only', \"there's\", 'those', 'under', 'ours', 'has', \n",
    "\"haven't\", 'do', 'them', 'his', \"they'll\", 'very', \"who's\", \"they'd\", \n",
    "'cannot', \"you've\", 'they', 'not', 'during', 'yourself', 'him', 'nor', \n",
    "\"we'll\", 'did', \"they've\", 'this', 'she', 'each', \"won't\", 'where', \n",
    "\"mustn't\", \"isn't\", \"i'll\", \"why's\", 'because', \"you'd\", 'doing', 'some', \n",
    "'up', 'are', 'further', 'ourselves', 'out', 'what', 'for', 'while', \n",
    "\"wasn't\", 'does', \"shouldn't\", 'above', 'between', 'be', 'we', 'who', \n",
    "\"you're\", 'were', 'here', 'hers', \"aren't\", 'by', 'both', 'about', 'would', \n",
    "'of', 'could', 'against', \"i'd\", \"weren't\", \"i'm\", 'or', \"can't\", 'own', \n",
    "'into', 'whom', 'down', \"hadn't\", \"couldn't\", 'your', \"doesn't\", 'from', \n",
    "\"how's\", 'her', 'their', \"it's\", 'there', 'been', 'why', 'few', 'too', \n",
    "'themselves', 'was', 'until', 'more', 'himself', \"where's\", \"i've\", 'with', \n",
    "\"didn't\", \"what's\", 'but', 'herself', 'than', \"here's\", 'he', 'me', \n",
    "\"they're\", 'myself', 'these', \"hasn't\", 'below', 'ought', 'theirs', 'my', \n",
    "\"wouldn't\", \"we'd\", 'and', 'then', 'is', 'am', 'it', 'an', 'as', 'itself', \n",
    "'at', 'have', 'in', 'any', 'if', 'again', 'no', 'that', 'when', 'same', \n",
    "'how', 'other', 'which', 'you', \"shan't\", 'our', 'after', \"let's\", 'most', \n",
    "'such', 'on', \"he'll\", 'a', 'off', 'i', \"she'd\", 'yours', \"you'll\", 'so', \n",
    "\"we're\", \"she's\", 'the', \"that's\", 'having', 'once'])\n",
    "\n",
    "def performance(y_true, pred, ann=True):\n",
    "    acc = accuracy_score(y_true, pred[:,1]>0.5)\n",
    "    auc = roc_auc_score(y_true, pred[:,1])\n",
    "    fpr, tpr, thr = roc_curve(y_true, pred[:,1])\n",
    "    plotfig, plotobj = plt.subplots(nrows=1, ncols=1, figsize=(16, 7))\n",
    "    plt.plot(fpr, tpr, color='royalblue', linewidth=\"3\")\n",
    "    plt.xlabel(\"False positive rate (FPR)\")\n",
    "    plt.ylabel(\"True positive rate (TPR)\")\n",
    "    if ann:\n",
    "        plotobj.annotate(\"Accuracy: %0.2f\" % acc, (0.2,0.7), size=14)\n",
    "        plotobj.annotate(\"AUC: %0.2f\" % auc, (0.2,0.6), size=14)\n",
    "\n",
    "def tokenize(docs):\n",
    "    pattern = re.compile('[\\W_]+', re.UNICODE)\n",
    "    sentences = []\n",
    "    for d in docs:\n",
    "        sentence = d.lower().split(\" \")\n",
    "        sentence = [pattern.sub('', w) for w in sentence]\n",
    "        sentences.append( [w for w in sentence if w not in stop_words] )\n",
    "    return sentences\n",
    "\n",
    "def w2v_feautrevec(model, sentences):\n",
    "    f = np.zeros((len(sentences), model.vector_size))\n",
    "    for i,s in enumerate(sentences):\n",
    "        for w in s:\n",
    "            try:\n",
    "                vec = model[w]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            f[i,:] = f[i,:] + vec\n",
    "        f[i,:] = f[i,:] / len(s)\n",
    "    return f\n",
    "\n",
    "def remove_nan(features):\n",
    "    rows_to_delete = []\n",
    "    for i in range(len(features)):\n",
    "        if np.isnan(features[i].sum()):\n",
    "            rows_to_delete.append(i)\n",
    "    return rows_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenize(d_train.comment_text)\n",
    "model = Word2Vec(sentences, size=500, window=5, min_count=6, sample=1e-3, workers=2)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "features = w2v_feautrevec(model, sentences)\n",
    "\n",
    "rows_to_delete = remove_nan(features)\n",
    "features = np.delete(features, rows_to_delete, 0)\n",
    "\n",
    "modelw2v = RandomForestClassifier(n_estimators=600, n_jobs=-1, max_features=\"log2\")\n",
    "modelw2v.fit(features, d_train.toxic.drop(d_train.index[rows_to_delete]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sent = tokenize(d_val.comment_text)\n",
    "features_val = featurize_w2v(model, validation_sent)\n",
    "\n",
    "deleterows = remove_nan(features_val)\n",
    "features_val = np.delete(features_val, deleterows, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = model3.predict_proba(features_val)\n",
    "performance(d_val.toxic.drop(d_val.index[deleterows]), pred3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Project_IDS_566 _V3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
